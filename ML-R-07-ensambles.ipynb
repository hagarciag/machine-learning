{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje de maquinas  -- R -- Metodos de ensamble.\n",
    "Notas de clase sobre aprendizaje de maquinas usando R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Juan David Velásquez Henao**   \n",
    "jdvelasq@unal.edu.co  \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia  \n",
    "\n",
    "[Licencia]\n",
    "\n",
    "[Readme]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Software utilizado**.\n",
    "\n",
    "> Este es un documento interactivo escrito como un notebook de [Jupyter](http://jupyter.org), en el cual se presenta un tutorial sobre regresión logistica usando **R** en el contexto de aprendizaje de maquinas. Los notebooks de Jupyter permiten incoporar simultáneamente código, texto, gráficos y ecuaciones. El código presentado en este notebook puede ejecutarse en los sistemas operativos Linux y OS X.\n",
    "\n",
    "> Haga click [aquí](https://github.com/jdvelasq/guias-de-instalacion) para obtener instrucciones detalladas sobre como instalar Jupyter en Windows y Mac OS X.\n",
    "\n",
    "> Haga clic [aquí] para ver la última versión de este documento en nbviewer.\n",
    "\n",
    "> Descargue la última versión de este documento a su disco duro; luego, carguelo y ejecutelo en línea en [Try Jupyter!](https://try.jupyter.org)\n",
    "\n",
    "> Haga clic [aquí](https://github.com/jdvelasq/ETVL-R/blob/master/ETVL-R-5-visualizacion-1-base.ipynb) para ver el tutorial de visualización y gráficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">* [Introducción](#Introducción)\n",
    "* [Bagging-Agregación Bootstraping](#Bagging-Agregación-Bootstraping)\n",
    "    * [Esquema de Pseudo-Algoritmo Bagging en Clasificación Binaria](#Esquema-de-Pseudo-Algoritmo-Bagging-en-Clasificación-Binaria)\n",
    "        * [Aplicación Bagging CART-SkillCraft](#Aplicación-Bagging-CART-SkillCraft)\n",
    "        * [Aplicación Bagging Regresión Logística-Enfermedades del Corazón](#Aplicación-Bagging-Regresión-Logística-Enfermedades-del-Corazón)\n",
    "* [Boosting](#Boosting)\n",
    "    * [Algoritmo AdaBoost en Clasificación Binaria](#Algoritmo-AdaBoost-en-Clasificación-Binaria)\n",
    "    * [Limitaciones del Boosting](#Limitaciones-del-Boosting)\n",
    "        * [Aplicación Boosting-AdaBoost](#Aplicación-Boosting-AdaBoost)\n",
    "* [Random Forest](#Random-Forest)\n",
    "    * [Aplicación Random Forest](#Aplicación-Random-Forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliografía**.\n",
    "\n",
    ">  \n",
    "\n",
    "**Material complementario.**\n",
    "> Webinar RStudio [Getting your data into R](https://www.rstudio.com/resources/webinars/getting-your-data-into-r/) \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "Para producir un modelo combinado que prediga de forma más precisa que cada modelo individual separado. En este notebook se abordarán tres métodos: **_Bagging_**, **_Boosting_** y **_Random Forest_**. Para esto se utilizarán modelos vistos en clases anteriores y cómo estos métodos mejoran el poder predictivo de estos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging-Agregación Bootstraping\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "Este primer método de ensamble conocido como Bagging se basa en utilizar diferentes muestras de todo el conjunto de observaciones para entrenar múltiples versiones del mismo modelo. Cada uno de estos modelos votan (arrojan su predicción) por la respuesta correcta donde gana o se escoje el valor de la predicción con mayores \"votos\" (es decir, por mayoría) cuando se trata de una variable de respuesta categórica. Análogamente para una variable de respuesta continua el valor de predicción es el promedio de todos los valores arrojados por los modelos.\n",
    "\n",
    "Básicamente el algoritmo conlleva a entrenar el mismo modelo $M$ veces cada una con diferentes datos de entrenamiento, a través de muestreo con reemplazo, y promediar los resultados de cada uno para tener una salida final. Esta técnica permite inferir que en promedio se obtiene el 63% de observaciones distintas cada vez que se realiza la muestra utilizando reemplazo.\n",
    "\n",
    "<img src=\"images/bagging.PNG\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esquema de Pseudo-Algoritmo Bagging en Clasificación Binaria\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "Dentro de este algoritmo las variables de entrada son la base completa de observaciones y la cantidad de modelos a entrenar $M$.\n",
    "\n",
    "1. Crear una muestra aleatoria de tamaño $n$ (tamaño de toda la base de datos). Es decir que quedaran observaciones repetidas y otras por fuera. Este proceso se conoce como **bootstraping**.\n",
    "\n",
    "2. Entrenar un modelo de clasificacion usando esta muestra de los datos. Usualmente, no es recomendable utilizar algortimos de regularización o modelos reducidos diseñados para combatir el sobre-ajuste ya que el proceso de agregación es usado al final es capaz de suavizar el ajuste.\n",
    "\n",
    "3. Para cada observacion en la muestra de los datos, se almacena la clase asignada por el modelo (0 o 1).\n",
    "\n",
    "4. Repertir este proceso $M$ veces para entrenar $M$ modelos.\n",
    "\n",
    "5. Para cada observacion en la base de datos original (sin muestreo), se calcula la predicción final de la clase a través del conteo de los resultados de cada modelo donde se escoge aquella con mayor número. Es decir, que si se entrenaron $M=25$, donde 20 de estos dicen que la clase es 0 y los otros 5 dicen que la clase es 1, la predicción final de dicha observación es 0.\n",
    "\n",
    "6. Calcular la precisión del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación Bagging CART-SkillCraft\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "\n",
    "Para el ejemplo con CART se utiliza el árbol que se generó en el notebook **XXXX** (Métodos Basados en Árboles), el cual utiliza la base de datos SkillCraft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [`rpart {rpart}`](https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.html)\n",
    "\n",
    "> [`caret {caret}`](ftp://cran.r-project.org/pub/R/web/packages/caret/caret.pdf)\n",
    "\n",
    "> [`ipred {ipred}`](https://cran.r-project.org/web/packages/ipred/ipred.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      ": package 'ipred' was built under R version 3.3.2"
     ]
    }
   ],
   "source": [
    "## Instale y cargue las siguientes librerías\n",
    "\n",
    "library(rpart)\n",
    "library(caret)\n",
    "library(ipred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In eval(expr, envir, enclos): NAs introducidos por coerciónWarning message:\n",
      "In eval(expr, envir, enclos): NAs introducidos por coerciónWarning message:\n",
      "In eval(expr, envir, enclos): NAs introducidos por coerción"
     ]
    }
   ],
   "source": [
    "## Codigos del capítulo 6. (Para obtener la descripcion de los codigos dirigirese al capitulo)\n",
    "\n",
    "set.seed(266)\n",
    "\n",
    "link                       <-\"https://archive.ics.uci.edu/ml/machine-learning-databases/00272/SkillCraft1_Dataset.csv\"\n",
    "skillcraft                 <-read.csv(url(link)) \n",
    "skillcraft                 <- skillcraft[-1] \n",
    "skillcraft$TotalHours      <- as.numeric(levels(skillcraft$TotalHours))[skillcraft$TotalHours]\n",
    "skillcraft$HoursPerWeek    <- as.numeric(levels(skillcraft$HoursPerWeek))[skillcraft$HoursPerWeek]\n",
    "skillcraft$Age             <- as.numeric(levels(skillcraft$Age))[skillcraft$Age]\n",
    "skillcraft                 <- skillcraft[complete.cases(skillcraft),]\n",
    "skillcraft_sampling_vector <- createDataPartition(skillcraft$LeagueIndex, p = 0.80, list = FALSE)\n",
    "skillcraft_train           <- skillcraft[skillcraft_sampling_vector,]\n",
    "skillcraft_test            <- skillcraft[-skillcraft_sampling_vector,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "684.919718694183"
      ],
      "text/latex": [
       "684.919718694183"
      ],
      "text/markdown": [
       "684.919718694183"
      ],
      "text/plain": [
       "[1] 684.9197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Bagging al árbol CART\n",
    "baggedtree <- bagging(LeagueIndex ~ .,                       # Formula del arbol\n",
    "                      data = skillcraft_train,               # Data de entrenamiento\n",
    "                      nbagg = 100,                           # Numero de replicas en bootstrapping \n",
    "                      coob = T)                              # calculo estimado out-of-bag estimate de la tasa de error\n",
    "\n",
    "## Predicción con el bagged tree\n",
    "baggedtree_predictions <- predict(baggedtree,                # Modelo CART bagged\n",
    "                                  skillcraft_test)           # Datos de validación\n",
    "\n",
    "## Función para calcular el SSE\n",
    "compute_SSE <- function(correct, predictions) { \n",
    "return(sum((correct - predictions) ^ 2))                     # Suma de los errores al cuadrado.\n",
    "}\n",
    "\n",
    "## Calculo error SSE\n",
    "(baggedtree_SSE <- compute_SSE(baggedtree_predictions,       # Valores pronosticados\n",
    "                               skillcraft_test$LeagueIndex)) # Valores de validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de  $SSE$ es de 684.92 el cual es menor que el arbol con mejoramiento de parámetros estimado en el capitlo 6 (701.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación Bagging Regresión Logística-Enfermedades del Corazón\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "\n",
    "El método de Bagging también se puede utilizar para una variedad de modelos. Para ilustrar esto, se utilizará el modelo de regresión logística que se realizo en libro 01 y evaluar la mejora del desempeño. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [`caret {caret}`](ftp://cran.r-project.org/pub/R/web/packages/caret/caret.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Instale y cargue las siguientes\n",
    "library(caret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Codigos del notebook - Regresión Logística (Para obtener la descripcion de los codigos dirigirese al notebook)\n",
    "\n",
    "heart                    <- read.table(\"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat\",\n",
    "                                           quote=\"\\\"\")\n",
    "\n",
    "names(heart)             <- c(\"AGE\", \"SEX\", \"CHESTPAIN\", \"RESTBP\", \"CHOL\",\n",
    "                              \"SUGAR\", \"ECG\", \"MAXHR\", \"ANGINA\", \"DEP\", \n",
    "                              \"EXERCISE\", \"FLUOR\",\"THAL\", \"OUTPUT\")\n",
    "\n",
    "heart$CHESTPAIN          <- factor(heart$CHESTPAIN)\n",
    "heart$ECG                <- factor(heart$ECG)\n",
    "heart$THAL               <- factor(heart$THAL)\n",
    "heart$EXERCISE           <- factor(heart$EXERCISE)\n",
    "heart$OUTPUT             <- heart$OUTPUT - 1\n",
    "\n",
    "\n",
    "set.seed(987954)\n",
    "\n",
    "heart_sampling_vector    <- createDataPartition(heart$OUTPUT, p = 0.85, list = FALSE)   \n",
    "heart_train              <- heart[heart_sampling_vector,]\n",
    "heart_train_labels       <- heart$OUTPUT[heart_sampling_vector] \n",
    "heart_test               <- heart[-heart_sampling_vector,]\n",
    "heart_test_labels        <- heart$OUTPUT[-heart_sampling_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con la base de datos cargada y depurada, se configura los parámetros de modelo de ensamble empezando por la obtención de los vectores de muestro aleatorio con reemplazo **bootstraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td> 10</td><td> 75</td><td>104</td><td>121</td><td> 14</td><td>219</td><td>229</td><td>189</td><td> 84</td><td>155</td><td>162</td></tr>\n",
       "\t<tr><td> 65</td><td>168</td><td>132</td><td>111</td><td>139</td><td>126</td><td> 58</td><td> 95</td><td>101</td><td>216</td><td> 74</td></tr>\n",
       "\t<tr><td> 25</td><td>203</td><td>130</td><td> 93</td><td>225</td><td> 93</td><td>219</td><td>139</td><td>185</td><td>186</td><td> 94</td></tr>\n",
       "\t<tr><td>151</td><td> 93</td><td>155</td><td>188</td><td>127</td><td>110</td><td>152</td><td>105</td><td>191</td><td> 14</td><td>106</td></tr>\n",
       "\t<tr><td> 85</td><td> 49</td><td> 89</td><td>135</td><td>105</td><td>192</td><td> 74</td><td>220</td><td>126</td><td>192</td><td>127</td></tr>\n",
       "\t<tr><td>181</td><td>200</td><td>212</td><td>182</td><td>169</td><td>165</td><td> 51</td><td> 68</td><td> 55</td><td> 13</td><td> 23</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllllllllll}\n",
       "\t  10 &  75 & 104 & 121 &  14 & 219 & 229 & 189 &  84 & 155 & 162\\\\\n",
       "\t  65 & 168 & 132 & 111 & 139 & 126 &  58 &  95 & 101 & 216 &  74\\\\\n",
       "\t  25 & 203 & 130 &  93 & 225 &  93 & 219 & 139 & 185 & 186 &  94\\\\\n",
       "\t 151 &  93 & 155 & 188 & 127 & 110 & 152 & 105 & 191 &  14 & 106\\\\\n",
       "\t  85 &  49 &  89 & 135 & 105 & 192 &  74 & 220 & 126 & 192 & 127\\\\\n",
       "\t 181 & 200 & 212 & 182 & 169 & 165 &  51 &  68 &  55 &  13 &  23\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "1. 10\n",
       "2. 65\n",
       "3. 25\n",
       "4. 151\n",
       "5. 85\n",
       "6. 181\n",
       "7. 75\n",
       "8. 168\n",
       "9. 203\n",
       "10. 93\n",
       "11. 49\n",
       "12. 200\n",
       "13. 104\n",
       "14. 132\n",
       "15. 130\n",
       "16. 155\n",
       "17. 89\n",
       "18. 212\n",
       "19. 121\n",
       "20. 111\n",
       "21. 93\n",
       "22. 188\n",
       "23. 135\n",
       "24. 182\n",
       "25. 14\n",
       "26. 139\n",
       "27. 225\n",
       "28. 127\n",
       "29. 105\n",
       "30. 169\n",
       "31. 219\n",
       "32. 126\n",
       "33. 93\n",
       "34. 110\n",
       "35. 192\n",
       "36. 165\n",
       "37. 229\n",
       "38. 58\n",
       "39. 219\n",
       "40. 152\n",
       "41. 74\n",
       "42. 51\n",
       "43. 189\n",
       "44. 95\n",
       "45. 139\n",
       "46. 105\n",
       "47. 220\n",
       "48. 68\n",
       "49. 84\n",
       "50. 101\n",
       "51. 185\n",
       "52. 191\n",
       "53. 126\n",
       "54. 55\n",
       "55. 155\n",
       "56. 216\n",
       "57. 186\n",
       "58. 14\n",
       "59. 192\n",
       "60. 13\n",
       "61. 162\n",
       "62. 74\n",
       "63. 94\n",
       "64. 106\n",
       "65. 127\n",
       "66. 23\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n",
       "[1,]  10   75  104  121   14  219  229  189   84  155   162  \n",
       "[2,]  65  168  132  111  139  126   58   95  101  216    74  \n",
       "[3,]  25  203  130   93  225   93  219  139  185  186    94  \n",
       "[4,] 151   93  155  188  127  110  152  105  191   14   106  \n",
       "[5,]  85   49   89  135  105  192   74  220  126  192   127  \n",
       "[6,] 181  200  212  182  169  165   51   68   55   13    23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Parámetros del modelo\n",
    "\n",
    "M     <- 11                                                          # Numero de modelos a entrenar\n",
    "seeds <- 70000 : (70000 + M - 1)                                     # Semillas de cada modelo\n",
    "n     <- nrow(heart_train)                                           # Número de observaciones\n",
    "\n",
    "## Generación de las posiciones del muestreo con reemplazo Bagging\n",
    "\n",
    "sample_vectors <- sapply(seeds,                                      # Para cada semilla hacemos\n",
    "                         function(x) {set.seed(x);                   # Establecer la semilla aleatoria\n",
    "                                      return(sample(n,               # Devolver el muestreo del 1 al n\n",
    "                                                    n,               # Una muestra de n elementos\n",
    "                                                    replace = T))    # Con reemplazo\n",
    "                                     }\n",
    "                        )\n",
    "head(sample_vectors)                                                 # Primeros valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez con la matriz de muestreo, se procede a entrenar los $M$ modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "\n",
       "Call:  glm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = data)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          AGE          SEX   CHESTPAIN2   CHESTPAIN3   CHESTPAIN4  \n",
       "  -7.594241    -0.020559     1.349963     1.731942     0.969115     3.077225  \n",
       "     RESTBP         CHOL        SUGAR         ECG1         ECG2        MAXHR  \n",
       "   0.028977     0.005318    -1.252289     0.533759    -0.197845    -0.021618  \n",
       "     ANGINA          DEP    EXERCISE2    EXERCISE3        FLUOR        THAL6  \n",
       "   1.292296     0.213861     1.745369     0.260860     1.592797    -0.782992  \n",
       "      THAL7  \n",
       "   2.201760  \n",
       "\n",
       "Degrees of Freedom: 229 Total (i.e. Null);  211 Residual\n",
       "Null Deviance:\t    314.9 \n",
       "Residual Deviance: 104.2 \tAIC: 142.2\n",
       "\n",
       "[[2]]\n",
       "\n",
       "Call:  glm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = data)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          AGE          SEX   CHESTPAIN2   CHESTPAIN3   CHESTPAIN4  \n",
       "   -5.25155     -0.03049      2.04531     -0.29309     -1.81503      2.42040  \n",
       "     RESTBP         CHOL        SUGAR         ECG1         ECG2        MAXHR  \n",
       "    0.04817      0.01083     -0.82672      1.09534      0.19889     -0.05267  \n",
       "     ANGINA          DEP    EXERCISE2    EXERCISE3        FLUOR        THAL6  \n",
       "    2.22879      0.37747     -0.17252      1.72625      2.01710      1.36969  \n",
       "      THAL7  \n",
       "    2.41236  \n",
       "\n",
       "Degrees of Freedom: 229 Total (i.e. Null);  211 Residual\n",
       "Null Deviance:\t    314.4 \n",
       "Residual Deviance: 82.88 \tAIC: 120.9\n",
       "\n",
       "[[3]]\n",
       "\n",
       "Call:  glm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = data)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          AGE          SEX   CHESTPAIN2   CHESTPAIN3   CHESTPAIN4  \n",
       "   -6.65939     -0.07101      3.01732      2.92833      2.00574      3.50170  \n",
       "     RESTBP         CHOL        SUGAR         ECG1         ECG2        MAXHR  \n",
       "    0.04140      0.01063     -2.51102      0.80422      0.34057     -0.03731  \n",
       "     ANGINA          DEP    EXERCISE2    EXERCISE3        FLUOR        THAL6  \n",
       "    1.73266      0.52395      0.64481      0.16485      1.63687      0.46386  \n",
       "      THAL7  \n",
       "    1.22768  \n",
       "\n",
       "Degrees of Freedom: 229 Total (i.e. Null);  211 Residual\n",
       "Null Deviance:\t    314.9 \n",
       "Residual Deviance: 121.2 \tAIC: 159.2\n",
       "\n",
       "[[4]]\n",
       "\n",
       "Call:  glm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = data)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          AGE          SEX   CHESTPAIN2   CHESTPAIN3   CHESTPAIN4  \n",
       "  -1.695002    -0.040329     1.048970     1.851488     1.694622     3.403773  \n",
       "     RESTBP         CHOL        SUGAR         ECG1         ECG2        MAXHR  \n",
       "   0.003866     0.005311    -0.083538     0.960724    -0.011692    -0.024451  \n",
       "     ANGINA          DEP    EXERCISE2    EXERCISE3        FLUOR        THAL6  \n",
       "   0.746202     0.665192    -0.204678    -1.098114     0.847238     2.829046  \n",
       "      THAL7  \n",
       "   1.568876  \n",
       "\n",
       "Degrees of Freedom: 229 Total (i.e. Null);  211 Residual\n",
       "Null Deviance:\t    313.2 \n",
       "Residual Deviance: 137.6 \tAIC: 175.6\n",
       "\n",
       "[[5]]\n",
       "\n",
       "Call:  glm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = data)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          AGE          SEX   CHESTPAIN2   CHESTPAIN3   CHESTPAIN4  \n",
       " -10.129915    -0.036380     2.652578     2.411853    -0.521962     3.804546  \n",
       "     RESTBP         CHOL        SUGAR         ECG1         ECG2        MAXHR  \n",
       "   0.021622     0.023483    -3.590200     0.916767    -0.319370    -0.026799  \n",
       "     ANGINA          DEP    EXERCISE2    EXERCISE3        FLUOR        THAL6  \n",
       "   1.018611    -0.007493     2.972216     1.772404     1.527764    -2.737567  \n",
       "      THAL7  \n",
       "   2.034268  \n",
       "\n",
       "Degrees of Freedom: 229 Total (i.e. Null);  211 Residual\n",
       "Null Deviance:\t    314.9 \n",
       "Residual Deviance: 108.2 \tAIC: 146.2\n",
       "\n",
       "[[6]]\n",
       "\n",
       "Call:  glm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = data)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          AGE          SEX   CHESTPAIN2   CHESTPAIN3   CHESTPAIN4  \n",
       " -13.674601    -0.002809     2.239275     2.558059    -0.146453     3.473136  \n",
       "     RESTBP         CHOL        SUGAR         ECG1         ECG2        MAXHR  \n",
       "   0.020230     0.006904    -1.562047     2.887265     0.687904     0.014562  \n",
       "     ANGINA          DEP    EXERCISE2    EXERCISE3        FLUOR        THAL6  \n",
       "   1.769353     0.413417     0.751045     1.608631     1.209291    -0.119684  \n",
       "      THAL7  \n",
       "   2.161520  \n",
       "\n",
       "Degrees of Freedom: 229 Total (i.e. Null);  211 Residual\n",
       "Null Deviance:\t    316.3 \n",
       "Residual Deviance: 109.6 \tAIC: 147.6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Funcion para realizar el modelo logístico\n",
    "train_1glm <- function(sample_indices) {                             # Funcion \"train_1glm\" que depende de las posiciones del muestreo\n",
    "    data   <- heart_train[sample_indices,];                          # subset la data con las posiciones de la muestra\n",
    "    model  <- glm(OUTPUT ~ .,                                        # Modelo con variable respuesta \"OUTPUT\"\n",
    "                 data = data,                                        # Datos de entrada\n",
    "                 family = binomial(\"logit\"));                        # Regresion Logistica\n",
    "    return(model)                                                    # La función devuelve el modelo\n",
    "}\n",
    "\n",
    "## Estimación los M modelos\n",
    "models    <- apply(sample_vectors,                                   # A cada vector de muestreo le aplicamos la función \n",
    "                2,                                                   # Indicador de aplicación por columnas\n",
    "                train_1glm)                                          # Función de modelo logístico\n",
    "\n",
    "head(models)                                                         # Primeros valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenados los modelos, se extrae la data de entrenamiento de cada modelo pero sin repeticiones lo cual servierá para hacer medidas de ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Funcion para obtener los valores de entrenamiento para cada modelo sin repetición\n",
    "\n",
    "get_1bag  <- function(sample_indices) {                              # Funcion \"get_1bag\" que depende de las posiciones del muestreo\n",
    "    unique_sample <- unique(sample_indices);                         # Posiciones unicas del muestreo\n",
    "    df    <- heart_train[unique_sample, ];                           # Observaciones de la muestra sin repetirse.\n",
    "    df$ID <- unique_sample;                                          # Agregar columna de identificador de la posisión de la muestra\n",
    "    return(df)                                                       # Mostrar el data.frame\n",
    "}\n",
    "\n",
    "## Se obtine los bags\n",
    "bags <- apply(sample_vectors,                                        # Vector de posiciones\n",
    "              2,                                                     # Aplicamos por columnas\n",
    "              get_1bag)                                              # Función de obtener los bags únicos\n",
    "\n",
    "#head(bags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción con los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Función para predecir con el modelo\n",
    "glm_predictions   <- function(model, data, model_index) {\n",
    "    colname       <- paste(\"PREDICTIONS\", \n",
    "                     model_index);\n",
    "    data[colname] <- as.numeric(\n",
    "        predict(model, \n",
    "                data, \n",
    "                type = \"response\") > 0.5);\n",
    "    return(data[,c(\"ID\", colname), \n",
    "                drop = FALSE])\n",
    "}\n",
    "\n",
    "## Predicciones de cada modelo\n",
    "training_predictions <- mapply(glm_predictions,                     # Función a aplicar\n",
    "                               models,                              # Cada uno de los 11 modelos \n",
    "                               bags,                                # Bags (Registros unicos de entrenamiento)\n",
    "                               1 : M,                               # Indice de cada modelo\n",
    "                               SIMPLIFY = F)                        # Devuelve una lista\n",
    "\n",
    "#head (training_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrupan todas las predicciones de cada modelo para cada registro. Esto se realiza haciendo un join entre cada predicción con la llave del ID de la fila, luego se reducen las filas y se coloca NA donde no haya predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ID</th><th scope=col>PREDICTIONS 1</th><th scope=col>PREDICTIONS 2</th><th scope=col>PREDICTIONS 3</th><th scope=col>PREDICTIONS 4</th><th scope=col>PREDICTIONS 5</th><th scope=col>PREDICTIONS 6</th><th scope=col>PREDICTIONS 7</th><th scope=col>PREDICTIONS 8</th><th scope=col>PREDICTIONS 9</th><th scope=col>PREDICTIONS 10</th><th scope=col>PREDICTIONS 11</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1 </td><td> 1</td><td>NA</td><td> 1</td><td>NA</td><td>NA</td><td> 1</td><td> 1</td><td>NA</td><td> 1</td><td> 1</td><td> 1</td></tr>\n",
       "\t<tr><td>2 </td><td> 0</td><td>NA</td><td>NA</td><td> 0</td><td>NA</td><td> 0</td><td> 1</td><td>NA</td><td>NA</td><td> 0</td><td>NA</td></tr>\n",
       "\t<tr><td>3 </td><td>NA</td><td> 0</td><td> 0</td><td>NA</td><td>NA</td><td> 0</td><td> 0</td><td> 0</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "\t<tr><td>4 </td><td>NA</td><td> 1</td><td> 1</td><td> 1</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td> 1</td><td> 1</td><td> 1</td></tr>\n",
       "\t<tr><td>5 </td><td> 0</td><td> 0</td><td> 0</td><td>NA</td><td> 0</td><td> 0</td><td> 0</td><td> 0</td><td> 0</td><td> 0</td><td>NA</td></tr>\n",
       "\t<tr><td>6 </td><td> 0</td><td> 1</td><td> 0</td><td> 0</td><td> 0</td><td>NA</td><td>NA</td><td>NA</td><td> 0</td><td>NA</td><td> 0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " ID & PREDICTIONS 1 & PREDICTIONS 2 & PREDICTIONS 3 & PREDICTIONS 4 & PREDICTIONS 5 & PREDICTIONS 6 & PREDICTIONS 7 & PREDICTIONS 8 & PREDICTIONS 9 & PREDICTIONS 10 & PREDICTIONS 11\\\\\n",
       "\\hline\n",
       "\t 1  &  1 & NA &  1 & NA & NA &  1 &  1 & NA &  1 &  1 &  1\\\\\n",
       "\t 2  &  0 & NA & NA &  0 & NA &  0 &  1 & NA & NA &  0 & NA\\\\\n",
       "\t 3  & NA &  0 &  0 & NA & NA &  0 &  0 &  0 & NA & NA & NA\\\\\n",
       "\t 4  & NA &  1 &  1 &  1 & NA & NA & NA & NA &  1 &  1 &  1\\\\\n",
       "\t 5  &  0 &  0 &  0 & NA &  0 &  0 &  0 &  0 &  0 &  0 & NA\\\\\n",
       "\t 6  &  0 &  1 &  0 &  0 &  0 & NA & NA & NA &  0 & NA &  0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  ID PREDICTIONS 1 PREDICTIONS 2 PREDICTIONS 3 PREDICTIONS 4 PREDICTIONS 5\n",
       "1 1   1            NA             1            NA            NA           \n",
       "2 2   0            NA            NA             0            NA           \n",
       "3 3  NA             0             0            NA            NA           \n",
       "4 4  NA             1             1             1            NA           \n",
       "5 5   0             0             0            NA             0           \n",
       "6 6   0             1             0             0             0           \n",
       "  PREDICTIONS 6 PREDICTIONS 7 PREDICTIONS 8 PREDICTIONS 9 PREDICTIONS 10\n",
       "1  1             1            NA             1             1            \n",
       "2  0             1            NA            NA             0            \n",
       "3  0             0             0            NA            NA            \n",
       "4 NA            NA            NA             1             1            \n",
       "5  0             0             0             0             0            \n",
       "6 NA            NA            NA             0            NA            \n",
       "  PREDICTIONS 11\n",
       "1  1            \n",
       "2 NA            \n",
       "3 NA            \n",
       "4  1            \n",
       "5 NA            \n",
       "6  0            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Agrupación de todas las predicciones en un data_frame\n",
    "train_pred_df <- Reduce(function(x, y) merge(x, y, by = \"ID\", all = T),   # Reducción de la matriz a unicamente el merge (JOIN)\n",
    "                        training_predictions)                             # Matriz a aplicar el Join y el Reduce\n",
    "    \n",
    "head(train_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 0\n",
       "3. 0\n",
       "4. 1\n",
       "5. 0\n",
       "6. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 0 0 1 0 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Resultado final del modelo Bagging\n",
    "train_pred_vote <- apply(train_pred_df,                                        # Data frame al cual le vamos a aplicar                                     \n",
    "                         1,                                                    # Aplicar por filas\n",
    "                         function(x) as.numeric(mean(x, na.rm = TRUE) > 0.5))  # Calcular la media de cada fila. Si la media es mayor que 0.5, la predicción es 1, de lo contrario 0.\n",
    "                             \n",
    "head(train_pred_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.447826086956522"
      ],
      "text/latex": [
       "0.447826086956522"
      ],
      "text/markdown": [
       "0.447826086956522"
      ],
      "text/plain": [
       "[1] 0.4478261"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## calculo de la precisión media del algoritmo\n",
    "(training_accuracy <- mean(train_pred_vote ==                                         # Valores predichos\n",
    "                           heart_train$OUTPUT[as.numeric(train_pred_df$ID)]))         # Valores reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.--** A partir de la base de datos de [clasificacion de vinos](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) utilice el bagging para entrenar una regresión logística y mejorar el desempeño de los resultados del modelo individual en categorizar cada uno dentro de las 3 clases. Parametrice un porcentaje de entramiento-validación de 90-10; tenga cuidad de no desbalancear las muestras.\n",
    "\n",
    "Las características de los vinos son:\n",
    "1. Alcohol \n",
    "2. Malic acid \n",
    "3. Ash \n",
    "4. Alcalinity of ash \n",
    "5. Magnesium \n",
    "6. Total phenols \n",
    "7. Flavanoids \n",
    "8. Nonflavanoid phenols \n",
    "9. Proanthocyanins \n",
    "10. Color intensity \n",
    "11. Hue \n",
    "12. OD280/OD315 of diluted wines \n",
    "13. Proline "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.--** Utilice la base de datos de detección de graude en tarjetas de credito en Septiembre de 2013. La descripción de la base de datos se puede enconrtar en la página de [Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud)\n",
    "\n",
    "Notas a la base de datos:\n",
    "_\"El conjunto de datos es altamente desequilibrado, la clase positiva (fraudes) representa el 0,172% de todas las transacciones.\n",
    "Contiene sólo variables numéricas de entrada que son el resultado de una transformación PCA. Lamentablemente, debido a problemas de confidencialidad, no podemos proporcionar las características originales y más información de fondo sobre los datos. Las características V1, V2, ... V28 son los componentes principales obtenidos con PCA, las únicas características que no han sido transformadas con PCA son 'Tiempo' y 'Cantidad'. La característica 'Tiempo' contiene los segundos transcurridos entre cada transacción y la primera transacción en el conjunto de datos. El campo 'Import' es la cantidad de la transacción. La característica 'Class' es la variable de respuesta y toma el valor 1 en caso de fraude y 0 en caso contrario.\n",
    "Dada la relación de desequilibrio de clase, recomendamos medir la precisión usando el área bajo la curva Precision-Recall (AUPRC). La precisión de la matriz de confusión no es significativa para la clasificación desequilibrada.\"_\n",
    "\n",
    "Para este ejercicio, se recomienda balancear la base de datos a un 1:5 o 1:6 (por cada 6 transacciones no fraudlentas, existe 1 fraudlenta). Puede utilizar cualquiera de las técnicas ya vistas en clase (regresión logística, redes neuronales, árboles de decisión). Lo importante es que utilice el concepto de bagging para mostrar la mejora en el desempeño de los modelos individuales.\n",
    "\n",
    "[Datos](https://drive.google.com/open?id=0B4psHlllKLPUTFNTM242MGhlM2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "\n",
    "El Boosting consiste en entrenar una cadena de modelos y asignarle pesos a las observaciones que fueron clasificadas incorrectamente o cayeron muy lejos de su valor esperado, de tal forma que los modelos siguientes estén forzados a priorizar dichas observaciones. \n",
    "\n",
    "Este metodo aforce una alternativa especialmente para aquellos algoritmos que son _\"débiles\"_, es decir, que producen una predicción un poco mejor que elección al azar. En estos modelos usualmente la complejidad es baja, no obstante, se pueden entrenar modelos cuyo parámetro de complejidad sea configurable como las redes neuronales o árbole de decisión.\n",
    "\n",
    "Una de las diferencias del Boosting respecto al bagging es que no existe un componente aleatorio al momento de elegir los datos de entranmiento. Todos los modelos se entrenan con la misma base de entrenamiento original. Otra diferencia es la cantidad de existente de tecnicas de _boosting_ para abordar los problemas (_**AdaBoost**_, BrownBoost, Stochastic Gradient Boost, CoBoost) mientras que el bagging no presenta técnicas con cambios importantes dentro de su implementación. \n",
    "\n",
    "De forma general, el boosting parte construyendo un modelo con las observaciones de entrenamiento y midiendo la precisión en los mismos datos. Cada una de las observaciones que fueron erroneamente identificadas por el modelo se les da un peso más grande de aquuellas que fueron correctamente clasificadas. De esta forma, el modelo se re-entrena usando estos pesos. Estos pasos se repiten multiples veces, ajustando nuevamente los pesos de los datos erroneamente categorizados en la iteración anterior.\n",
    "\n",
    "Lógicamente, si este proceso sigue indefinidamente el modelo de la iteración final estará sobre-ajustado. Por lo tanto, para evitar este inconveniente, el modelo ensamblado se construye a partir de el promedio ponderado (usualmente son proporcionales a la precisión) de todos los modelos entrenados en el proceso. Es decir, desde el modelo inicial hasta el modelo final de todo el proceso de ajuste de pesos. Usualmente, en modelos de regresión se ajustan los pesos de las observaciones en base a alguna médida de distancia entre el valor predicho y el valor real. \n",
    "\n",
    "<img src=\"images/boosting.PNG\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo AdaBoost en Clasificación Binaria\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "Existen dos tipos Adaptative Boost (AdaBoost): Discreta (binaria) y Real (multinomial). No obstante también existen extensiones para problemas de regresión. El input de este tipo de algoritmos son los mismos que en el bagging, la base completa de observaciones y la cantidad de modelos a entrenar _M_.\n",
    "\n",
    "1. Se inicia el vector de pesos de cada observación, $w$, de longitud $n$ (numero de observaciones) con el valor de $ w_i = \\frac {1}{n} $. Estos valores son actualizados en cada iteración\n",
    "\n",
    "2. Se usa el vector actual de pesos y todos los datos para entrenar un modelo $ G_m $.\n",
    "\n",
    "3. Se calcula la tasa de error ponderado como la suma de todas las observaciones mal clasificadas multiplicada por su peso, dividido por la suma del vector de pesos. Esto se expresa como:\n",
    "\n",
    "$$ err_m = \\frac {\\sum_{i=1}^{n} w_i · I(y_i \\neq G_m (x))}{\\sum_{i=1}^{n} w_i} $$\n",
    "\n",
    "4. Luego se transforma el peso para este modelo, $ a_m $, como el logaritmo de la división entre la la precisión y el error. Lo anterior se expresa:\n",
    "\n",
    "$$ a_m = \\frac {1}{2} · log_e (\\frac {1-err_m}{err_m}) $$\n",
    "\n",
    "5. Se actualizan los pesos de las observaciones $w$ para la próxima iteracion. Los eventos de observaciones mal clasificadas se multiplican su peso actual por $e^{a_m}$, lo que hace que incremente dicho peso. Por el contrario, aquellas que fueron bien clasificadas, se multiplican por $e^{-a_m}$, reduciendo su peso dentro del modelo.\n",
    "\n",
    "6. Se renormaliza los pesos del vector de tal forma que la suma de ellos den 1.\n",
    "\n",
    "7. Repetir los pasos del 2 al 6, _M_ veces para producir _M_ modelos.\n",
    "\n",
    "8. Se define el modelo final como la [función signo](https://es.wikipedia.org/wiki/Funci%C3%B3n_signo) de la suma ponderada de las salidas de todos los modelos $ G_m $, luego:\n",
    "\n",
    "$$ G(x) = sign(\\sum_{m=1}^{M} a_m · G_m(x) )$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitaciones del Boosting\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "- En ocasiones la agregación de algoritmos débiles puede generar un sobre ajuste en el modelo.\n",
    "- La mayoría de los algoritmos tienen una función de ajuste de pesos simétrica para el error, donde en cierto tipos de problemas puede ser más importante el error tipo I que el error tipo II, o viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación Boosting-AdaBoost\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "En este ejemplo se utiliza los datos del telescopio de rayos gamma donde se analiza los patrones de onda para predecir si un patron en particular viene de rayos gamas que se filtran en la atmosfera o vienen de radiación de fondo normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [`caret {caret}`](ftp://cran.r-project.org/pub/R/web/packages/caret/caret.pdf)\n",
    "\n",
    "> [`nnet {nnet}`](https://cran.r-project.org/web/packages/nnet/nnet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Instale y cargue las siguientes librerías\n",
    "library(caret)\n",
    "library(nnet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>FLENGTH</th><th scope=col>FWIDTH</th><th scope=col>FSIZE</th><th scope=col>FCONC</th><th scope=col>FCONC1</th><th scope=col>FASYM</th><th scope=col>FM3LONG</th><th scope=col>FM3TRANS</th><th scope=col>FALPHA</th><th scope=col>FDIST</th><th scope=col>CLASS</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 28.7967</td><td> 16.0021</td><td>2.6449  </td><td>0.3918  </td><td>0.1982  </td><td> 27.7004</td><td> 22.0110</td><td> -8.2027</td><td>40.0920 </td><td> 81.8828</td><td>1       </td></tr>\n",
       "\t<tr><td> 31.6036</td><td> 11.7235</td><td>2.5185  </td><td>0.5303  </td><td>0.3773  </td><td> 26.2722</td><td> 23.8238</td><td> -9.9574</td><td> 6.3609 </td><td>205.2610</td><td>1       </td></tr>\n",
       "\t<tr><td>162.0520</td><td>136.0310</td><td>4.0612  </td><td>0.0374  </td><td>0.0187  </td><td>116.7410</td><td>-64.8580</td><td>-45.2160</td><td>76.9600 </td><td>256.7880</td><td>1       </td></tr>\n",
       "\t<tr><td> 23.8172</td><td>  9.5728</td><td>2.3385  </td><td>0.6147  </td><td>0.3922  </td><td> 27.2107</td><td> -6.4633</td><td> -7.1513</td><td>10.4490 </td><td>116.7370</td><td>1       </td></tr>\n",
       "\t<tr><td> 75.1362</td><td> 30.9205</td><td>3.1611  </td><td>0.3168  </td><td>0.1832  </td><td> -5.5277</td><td> 28.5525</td><td> 21.8393</td><td> 4.6480 </td><td>356.4620</td><td>1       </td></tr>\n",
       "\t<tr><td> 51.6240</td><td> 21.1502</td><td>2.9085  </td><td>0.2420  </td><td>0.1340  </td><td> 50.8761</td><td> 43.1887</td><td>  9.8145</td><td> 3.6130 </td><td>238.0980</td><td>1       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       " FLENGTH & FWIDTH & FSIZE & FCONC & FCONC1 & FASYM & FM3LONG & FM3TRANS & FALPHA & FDIST & CLASS\\\\\n",
       "\\hline\n",
       "\t  28.7967 &  16.0021 & 2.6449   & 0.3918   & 0.1982   &  27.7004 &  22.0110 &  -8.2027 & 40.0920  &  81.8828 & 1       \\\\\n",
       "\t  31.6036 &  11.7235 & 2.5185   & 0.5303   & 0.3773   &  26.2722 &  23.8238 &  -9.9574 &  6.3609  & 205.2610 & 1       \\\\\n",
       "\t 162.0520 & 136.0310 & 4.0612   & 0.0374   & 0.0187   & 116.7410 & -64.8580 & -45.2160 & 76.9600  & 256.7880 & 1       \\\\\n",
       "\t  23.8172 &   9.5728 & 2.3385   & 0.6147   & 0.3922   &  27.2107 &  -6.4633 &  -7.1513 & 10.4490  & 116.7370 & 1       \\\\\n",
       "\t  75.1362 &  30.9205 & 3.1611   & 0.3168   & 0.1832   &  -5.5277 &  28.5525 &  21.8393 &  4.6480  & 356.4620 & 1       \\\\\n",
       "\t  51.6240 &  21.1502 & 2.9085   & 0.2420   & 0.1340   &  50.8761 &  43.1887 &   9.8145 &  3.6130  & 238.0980 & 1       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  FLENGTH  FWIDTH   FSIZE  FCONC  FCONC1 FASYM    FM3LONG  FM3TRANS FALPHA \n",
       "1  28.7967  16.0021 2.6449 0.3918 0.1982  27.7004  22.0110  -8.2027 40.0920\n",
       "2  31.6036  11.7235 2.5185 0.5303 0.3773  26.2722  23.8238  -9.9574  6.3609\n",
       "3 162.0520 136.0310 4.0612 0.0374 0.0187 116.7410 -64.8580 -45.2160 76.9600\n",
       "4  23.8172   9.5728 2.3385 0.6147 0.3922  27.2107  -6.4633  -7.1513 10.4490\n",
       "5  75.1362  30.9205 3.1611 0.3168 0.1832  -5.5277  28.5525  21.8393  4.6480\n",
       "6  51.6240  21.1502 2.9085 0.2420 0.1340  50.8761  43.1887   9.8145  3.6130\n",
       "  FDIST    CLASS\n",
       "1  81.8828 1    \n",
       "2 205.2610 1    \n",
       "3 256.7880 1    \n",
       "4 116.7370 1    \n",
       "5 356.4620 1    \n",
       "6 238.0980 1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Lectura de datos\n",
    "magic        <- read.csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\",   \n",
    "                   header=FALSE)                                                  # Sin encabezados\n",
    "names(magic) <- c(\"FLENGTH\", \"FWIDTH\", \"FSIZE\", \"FCONC\", \"FCONC1\",\"FASYM\",\n",
    "                  \"FM3LONG\", \"FM3TRANS\", \"FALPHA\", \"FDIST\", \"CLASS\")              # Nombres columnas\n",
    "\n",
    "magic$CLASS  <- as.factor(ifelse(magic$CLASS =='g', 1, -1))                       # Conversion CLASS a factor (-1,1)\n",
    "\n",
    "head(magic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creación datasets de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Datos de training y test \n",
    "\n",
    "set.seed(33711209)                                            # Semilla de aleatoriedad\n",
    "\n",
    "magic_sampling_vector <- createDataPartition(magic$CLASS,     # Posiciones de variable CLASS\n",
    "                                             p = 0.80,        # Proporcion de set de training\n",
    "                                             list = FALSE)    # Devolver como lista es FALSO\n",
    "\n",
    "magic_train           <- magic[magic_sampling_vector, 1:10]   # Subset de la data de training sin CLASS (Predictoras)\n",
    "magic_train_output    <- magic[magic_sampling_vector, 11]     # Subset de la data de training de CLASS (Respuesta)\n",
    "magic_test            <- magic[-magic_sampling_vector, 1:10]  # Subset de la data de test sin CLASS (Predictoras)\n",
    "magic_test_output     <- magic[-magic_sampling_vector, 11]    # Subset de la data de test de CLASS (Respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización de los datos con media igual a cero (0) y desviación estándar igual a uno (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Transoformacion de los datos\n",
    "magic_pp            <- preProcess(magic_train,           # Generar un modelo de normalización estándar.\n",
    "                       method = c(\"center\", \"scale\"))    # Center = Media 0, Scale = Division por la desviación\n",
    "\n",
    "magic_train_pp      <- predict(magic_pp,                 # Normalización de la data de entrenamiento\n",
    "                          magic_train)\n",
    "\n",
    "magic_train_df_pp   <- cbind(magic_train_pp,             # Juntar data de entrenamiento normalizada y la variable respuesta\n",
    "                           CLASS = magic_train_output)\n",
    "\n",
    "magic_test_pp       <- predict(magic_pp,                 # Normalización de la data de test\n",
    "                         magic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de una red neuronal con una capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  13\n",
      "initial  value 9853.413883 \n",
      "iter  10 value 7272.941211\n",
      "iter  20 value 7099.756392\n",
      "iter  30 value 7006.837619\n",
      "iter  40 value 6999.147893\n",
      "iter  50 value 6984.058978\n",
      "iter  60 value 6983.258778\n",
      "iter  70 value 6981.254951\n",
      "iter  80 value 6979.846315\n",
      "iter  90 value 6979.678059\n",
      "iter 100 value 6979.312668\n",
      "final  value 6979.312668 \n",
      "stopped after 100 iterations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.795424664738364"
      ],
      "text/latex": [
       "0.795424664738364"
      ],
      "text/markdown": [
       "0.795424664738364"
      ],
      "text/plain": [
       "[1] 0.7954247"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Entrenamos un modelo de red neural con una capa de la misma forma que el capitulo 4.\n",
    "                                                             \n",
    "n_model            <- nnet(CLASS ~ .,                                     # Variable respuesta CLASS, predictoras el resto\n",
    "                            data = magic_train_df_pp,                     # Data de entrenamiento normalizada\n",
    "                            size = 1)                                     # Tamaño de las capas ocultas\n",
    "\n",
    "n_test_predictions <- predict(n_model,                                     # Predecimos con la red neuronal\n",
    "                              magic_test_pp,                               # Data de test\n",
    "                              type = \"class\")                              # Predicción de CLASS (NO PROBABILIDAD)\n",
    "\n",
    "## calculo de la precisión media del algoritmo\n",
    "(n_test_accuracy <- mean(n_test_predictions == magic_test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de las propias funciones personalizadas para implementar AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creamos la función AdaBoost\n",
    "AdaBoostNN <- function(training_data, output_column, M, hidden_units) {  # Funcion AdaBoost (Datos entrenamiento, Variable respuesta, Numero modelos, numero capas escondidas)\n",
    "    \n",
    "    require(\"nnet\")                                                         # Libreria\n",
    "    models <- list()                                                        # lista vacia de modelos\n",
    "    alphas <- list()                                                        # lista vacia de alphas\n",
    "    n <- nrow(training_data)                                                # número de observaciones para entrenar\n",
    "    model_formula <- as.formula(paste(output_column, '~ .', sep = ''))      # fórmula (Variable respuesta ~ Variables Predictoras)\n",
    "    w <- rep((1/n), n)                                                      # Vector de pesos iniciales proporcionales \n",
    "    \n",
    "## Ciclo para los M modelos\n",
    "    for (m in 1:M) {                                                        # Ciclo for de 1 a M (numero modelos)\n",
    "## Generar el modelo y sus predicciones\n",
    "        model <- nnet(model_formula,                                        # Formula\n",
    "                      data = training_data,                                 # Datos de training\n",
    "                      size = hidden_units,                                  # Capas ocultas\n",
    "                      weights = w)                                          # Pesos\n",
    "        models[[m]] <- model                                                # Guardar modelo en la lista\n",
    "        predictions <- as.numeric(predict(model,                            # Forzar a numerico la predicción delo modelo\n",
    "            training_data[, -which(names(training_data) ==output_column)],  # Data training (Sin la variable respuesta)\n",
    "                                          type = \"class\"))                  # Prediccion de clase (no de probabilidad)\n",
    "## calculo de los errores para ajuste y su corrección     \n",
    "        errors <- predictions != training_data[, output_column]             # Comparación de las predicciones con los reales\n",
    "        error_rate <- sum(w * as.numeric(errors)) / sum(w)                  # Calculo de la tasa de error\n",
    "        alpha <- 0.5 * log((1 - error_rate) / error_rate)                   # Calculo de los alpha de correccion\n",
    "        alphas[[m]] <- alpha                                                # Se almace el alpha\n",
    "        temp_w <- mapply(                                                   # Aplicación de la funcion\n",
    "            function(x, y) if (y) { x * exp(alpha) }                        # Si esta bien clasificada, su peso es con alpha positivo\n",
    "                else { x * exp(-alpha)},                                    # De lo contrario, su peso es con alpha negativo\n",
    "            w,                                                              # Use los pesos actuales como x\n",
    "            errors)                                                         # Use los errores como y\n",
    "        w <- temp_w / sum(temp_w)                                           # Normalice los pesos\n",
    "    }\n",
    "## Termina el for\n",
    "            \n",
    "return(list(models = models, alphas = unlist(alphas)))                      # Devuelva los modelos, los alphas.\n",
    "}\n",
    "\n",
    "## Creacion función para predecir con AdaBoost\n",
    "AdaBoostNN.predict <- function(ada_model, test_data) {                      # Función recibe modelo y data test\n",
    "    models <- ada_model$models                                              # Asignación modelos\n",
    "    alphas <- ada_model$alphas                                              # Asignacion alphas\n",
    "    \n",
    "## Creacion matriz de predicciones\n",
    "    prediction_matrix <- sapply(models,                                     # Para cada modelo\n",
    "                                function (x) as.numeric(predict(x,          # Forzar a numerico las predicciones \n",
    "                                                                test_data,  # Con la data de validacion\n",
    "                                                                type = \"class\")))  # Forzar a salida clase\n",
    "                                    \n",
    "## Calculo de predicciones ponderadas\n",
    "    weighted_predictions <- t(apply(prediction_matrix,                           # Transponer el apply de la matriz de prediccion = x \n",
    "                                    1,                                           # Por filas\n",
    "                                    function(x) mapply(function(y, z) y * z,     # de la multiplicación y * z\n",
    "                                        x,                                       # Donde y son las predicciones\n",
    "                                        alphas)))                                # z son los alphas\n",
    "## Aplicacion de la función signo (forza -1 o 1)                                       \n",
    "    final_predictions <- apply(weighted_predictions,                             # Aplicacion a las predicciones ponderadas\n",
    "                               1,                                                # Por filas\n",
    "                               function(x) sign(sum(x)))                         # La función signo\n",
    "                                   \n",
    "    return(final_predictions)                                                    # Devolver las predicciones\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza las funciones anteriores para entrenar la red neuronal con boosting y predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  13\n",
      "initial  value 0.653043 \n",
      "iter  10 value 0.490532\n",
      "iter  20 value 0.472345\n",
      "iter  30 value 0.467543\n",
      "iter  40 value 0.465317\n",
      "iter  50 value 0.462195\n",
      "iter  60 value 0.461556\n",
      "iter  70 value 0.461182\n",
      "iter  80 value 0.460487\n",
      "iter  90 value 0.460309\n",
      "iter 100 value 0.460104\n",
      "final  value 0.460104 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.691335 \n",
      "iter  10 value 0.682753\n",
      "iter  20 value 0.673642\n",
      "iter  30 value 0.669705\n",
      "iter  40 value 0.668436\n",
      "iter  50 value 0.667255\n",
      "iter  60 value 0.665946\n",
      "iter  70 value 0.665716\n",
      "iter  80 value 0.665105\n",
      "iter  90 value 0.664161\n",
      "iter 100 value 0.663563\n",
      "final  value 0.663563 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.694629 \n",
      "iter  10 value 0.692252\n",
      "iter  20 value 0.677093\n",
      "iter  30 value 0.663049\n",
      "iter  40 value 0.661402\n",
      "iter  50 value 0.652622\n",
      "iter  60 value 0.643111\n",
      "iter  70 value 0.636908\n",
      "iter  80 value 0.636534\n",
      "iter  90 value 0.635478\n",
      "iter 100 value 0.635054\n",
      "final  value 0.635054 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.690733 \n",
      "iter  10 value 0.682223\n",
      "iter  20 value 0.667209\n",
      "iter  30 value 0.653828\n",
      "iter  40 value 0.642063\n",
      "iter  50 value 0.634364\n",
      "iter  60 value 0.631926\n",
      "iter  70 value 0.631224\n",
      "iter  80 value 0.629839\n",
      "iter  90 value 0.629304\n",
      "iter 100 value 0.629026\n",
      "final  value 0.629026 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.689411 \n",
      "iter  10 value 0.675165\n",
      "iter  20 value 0.665723\n",
      "iter  30 value 0.665082\n",
      "iter  40 value 0.661048\n",
      "iter  50 value 0.652168\n",
      "iter  60 value 0.649418\n",
      "iter  70 value 0.646686\n",
      "iter  80 value 0.644873\n",
      "iter  90 value 0.644294\n",
      "iter 100 value 0.644020\n",
      "final  value 0.644020 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.696713 \n",
      "iter  10 value 0.692991\n",
      "iter  20 value 0.691617\n",
      "iter  30 value 0.690348\n",
      "iter  40 value 0.689315\n",
      "iter  50 value 0.674067\n",
      "iter  60 value 0.668855\n",
      "iter  70 value 0.665634\n",
      "iter  80 value 0.661327\n",
      "iter  90 value 0.660405\n",
      "iter 100 value 0.659419\n",
      "final  value 0.659419 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.737065 \n",
      "iter  10 value 0.687170\n",
      "iter  20 value 0.681067\n",
      "iter  30 value 0.671485\n",
      "iter  40 value 0.666182\n",
      "iter  50 value 0.662185\n",
      "iter  60 value 0.657805\n",
      "iter  70 value 0.656860\n",
      "iter  80 value 0.655014\n",
      "iter  90 value 0.653984\n",
      "iter 100 value 0.653356\n",
      "final  value 0.653356 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.723157 \n",
      "iter  10 value 0.683728\n",
      "iter  20 value 0.681521\n",
      "iter  30 value 0.680551\n",
      "iter  40 value 0.679805\n",
      "iter  50 value 0.678369\n",
      "iter  60 value 0.677566\n",
      "iter  70 value 0.674697\n",
      "iter  80 value 0.670483\n",
      "iter  90 value 0.667999\n",
      "iter 100 value 0.667133\n",
      "final  value 0.667133 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.736406 \n",
      "iter  10 value 0.684576\n",
      "iter  20 value 0.678103\n",
      "iter  30 value 0.675963\n",
      "iter  40 value 0.673703\n",
      "iter  50 value 0.666030\n",
      "iter  60 value 0.664828\n",
      "iter  70 value 0.663762\n",
      "iter  80 value 0.663533\n",
      "iter  90 value 0.663429\n",
      "iter 100 value 0.663391\n",
      "final  value 0.663391 \n",
      "stopped after 100 iterations\n",
      "# weights:  13\n",
      "initial  value 0.687378 \n",
      "iter  10 value 0.684246\n",
      "iter  20 value 0.684055\n",
      "iter  30 value 0.680901\n",
      "iter  40 value 0.680098\n",
      "iter  50 value 0.679991\n",
      "iter  60 value 0.679894\n",
      "iter  70 value 0.679710\n",
      "iter  80 value 0.679645\n",
      "iter  90 value 0.678764\n",
      "iter 100 value 0.675372\n",
      "final  value 0.675372 \n",
      "stopped after 100 iterations\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in AdaBoostNN.predict(ada_model, magic_test_pp, \"CLASS\"): unused argument (\"CLASS\")\n",
     "output_type": "error",
     "traceback": [
      "Error in AdaBoostNN.predict(ada_model, magic_test_pp, \"CLASS\"): unused argument (\"CLASS\")\nTraceback:\n",
      "1. AdaBoostNN.predict(ada_model, magic_test_pp, \"CLASS\")"
     ]
    }
   ],
   "source": [
    "## Ejecutar el modelos con Boosting y predecimos\n",
    "ada_model <- AdaBoostNN(magic_train_df_pp,                   # Datos de entrenamiento\n",
    "                        'CLASS',                             # Variable respuesta\n",
    "                        10,                                  # Numero de modelos a entrenar\n",
    "                        1)                                   # Numero de capas ocultas de la red\n",
    "\n",
    "predictions <- AdaBoostNN.predict(ada_model,                 # Prediccion con el modelo\n",
    "                                  magic_test_pp,             # Datos de testing\n",
    "                                  'CLASS')                   # Variable respuesta\n",
    "\n",
    "## Calculo la precisión media del algoritmo\n",
    "mean(predictions == magic_test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.--** Utilice la base de datos de sobrevientes del Titanic donde ajuste un modelo de clasificación para predecir si una persona sobrevivió al Titanic a partir de sus características. Mediante estas técnicas mas avanzadas, que modelo tiene mejor desempeño respecto al modelo de regresión logistica realizado al inicio del curso?\n",
    "\n",
    "[Datos](https://drive.google.com/file/d/0B4psHlllKLPUc2tEVWxsM1hLbUE/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "\n",
    "Los _**bosques aleatorios (Random Forest)**_ es una técnica de ensamble basada en el concepto de árbol que se estudió en el notebook **XXXX**. Básicamente, la idea se deriva de una situación particular en bagged trees (árboles con bagging). Si se supone que la relación entre las variables predictoras y la respuesta se puede modelar correctamente con un árbol de decisión, es muy probable que dentro del proceso de bagging sigamos escogiendo las mismas variables para particionar las observaciones en todos los modelos. Esto conlleva a que todos los árboles no sean independientes uno de los otros porque tendrán los mismos nodos y valores, por lo tanto el promedio de los resultados será menos exitoso al tratar de reducir la vairanza en el ensamblaje. \n",
    "\n",
    "Para atacar esto, el algoritmo de bosques aleatorios sigue con el concepto de bagged trees e introduce un elemento de aleatoriedad en el proceso de construcción del arboles imponiendo una restricción. Para cada nodo en el arbol, se extrae una muesta aleatoria de tamaño $ m_{try} $ del total de variables predictoras (se quitan variables) y se determina la partición con esta muestra. Lo anterior asegura que las variables más importantes son muestreadas de forma suficiente.\n",
    "\n",
    "<img src=\"images/rf.jpg\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación Random Forest\n",
    "[Contenido](#Contenido)\n",
    "\n",
    "\n",
    "Dentro del ejemplo de Random Forest,se la base de datos de SkillCraft de la clase anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [`e1071 {e1071}`](https://cran.r-project.org/web/packages/e1071/index.html)\n",
    "\n",
    "> [`randomForest {randomForest}`](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      ": package 'randomForest' was built under R version 3.3.2randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "Warning message:\n",
      ": package 'e1071' was built under R version 3.3.2"
     ]
    }
   ],
   "source": [
    "## Instale y cargue las siguiente librerias\n",
    "library(randomForest)\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## calibracion del modelo\n",
    "rf_ranges <- list(ntree = c(500, 1000, 1500, 2000),       # Busqueda del mejor número de arboles en el modelo \n",
    "                  mtry = 3:8)                             # Intentos para cada número de arboles\n",
    "\n",
    "rf_tune   <- tune(randomForest,                           # Metodo a calibrar, en este caso RandomForest\n",
    "                LeagueIndex ~ .,                          # variable respuesta ~ Variable predictoras\n",
    "                data =skillcraft_train,                   # Datos de entrenamiento\n",
    "                ranges = rf_ranges)                       # Rangos de búsqueda\n",
    "\n",
    "rf_tune$best.parameters\n",
    "\n",
    "## Modelo final calibrado y predicciones\n",
    "\n",
    "rf_best             <- rf_tune$best.model                 # Extraer el mejor modelo\n",
    "\n",
    "rf_best_predictions <- predict(rf_best,                   # Predecimos con el mejor modelo\n",
    "                               skillcraft_test)           # Data de testing\n",
    "\n",
    "## Calculo SSE e importancia\n",
    "(rf_best_SSE <- compute_SSE(rf_best_predictions,          # Calculo del SSE (Valores predichos vs Valores reales)\n",
    "                            skillcraft_test$LeagueIndex))\n",
    "\n",
    "importance(rf_tune)                                       # Grafico la importancia de cada variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.--**  Se busca predecir el precio de la gasolina dadas las características en la base de datos [gasolina](https://www.datos.gov.co/api/views/7pcy-5vx9/rows.csv?accessType=DOWNLOAD). Utilice el método de Random Forest para abarcar dicho problema.\n",
    "\n",
    "El desarrollo y resultado del ejercicio busca responder:\n",
    "\n",
    "- ¿En qué zonas del país la gasolina es más cara y más barata?\n",
    "- ¿Qué gasolina (Porducto) son más baratas para comprar combustible?\n",
    "- ¿Cuales son los distribuidores de gasolina (bandera) con precios más competitivos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Contenido](#Contenido)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
